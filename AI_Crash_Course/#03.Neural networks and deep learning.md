AlexNet: a NN ,used a lot of hidden layers; used faster computation hardware to handle all the math that neural networks do.
AlexNet sparked an explosion of research into neural networks
UNDERSTAND NEURAL NETWORKS’ ARCHITECTURE
an input layer:  where the neural network recieves data represented as numbers.Each input neuron represents a single **feature(numbers)**,  which is some characteristic of the data.

- Once the features have data, each one sends its number to every neuron in the next layer.

anything can be converted to a number
an output layer 

any number of hidden layers in between
an input layer:  where the neural network recieves data represented as numbers.Each input neuron represents a single **feature(numbers)**,  which is some characteristic of the data.

- Once the features have data, each one sends its number to every neuron in the next layer.

anything can be converted to a number

an output layer 

any number of hidden layers in between
A neural network kind of seems like a black box that does math and spits out an answer.

Those middle layers are even called hidden layers!

HOW IT WORKS?

1.show a photo(every feature contain a number between 0 and 1 corresponding to the brightness of one pixel)

2.pass the information to the hidden layer

[since the neural network is already trained, this neuron has a mathematical formula to look for a particular component in the image, like a specific curve in the center.]

Focuse on specific shape and spot→may not really care what’s happening everywhere else→multiply or weight the pixel values from most of those features by 0 or close to 0.

Looking for bright pixels→multiply bright pixel values by a positive weight

Curve defined by a darker part below→multiply darker pixel values by a negative weight→add all the weighted pixel values from the input neurons and squish the results so that it’s between 0 and 1

THE FINAL NUMBER : basically represents the guess of this neuron thinking that a specific curve appear in the image→a dog nose

All the neurons pass their estimates onto the next hidden layer and are trained to look for more complex components.

WEIGHT VALUES AND ADD UP

People are excited about using deeper neural networks, which are networks with more hidden layers, to do deep learning.

DEEP LEARNING: deep networks can conbine input data in more complex ways to look for more complex components, and solve trickier problems.

AS a network get deeper, it gets harder for us to make sense of WHY it’s giving the answers it does.


