①Natural Language Understanding: how we get meaning out of combinations of letters.

②Natural Language Generation: how to generate language from knowledge. 

How did we learn to attach meaning to sounds?

How do we know great enthusiastic means something different from great sarcastic?  

There are a lot of related words that have a lot in common while some that don't.

DISTRIBUTIONAL SEMANTICS分布语义: guessing words that have similar meaning or seeing which words appear in the same sentences a lot. 

COUNT VECTORS: the number of times a word appears in the same article or sentence as other common words. 

If two words show up in the same sentence, they probably have pretty similar meanings.
drawback: need a lot of data.

ENCODER-DECODER MODEL: 

the encoder tells us what we should think and remember about what we just read…[read the input]

the decoder uses that thought to decide what we want to say or do.

Language modeling

RECURRENT 递归 NEURAL NETWORK(RNN): RNNs have a loop to let them reuse a single hidden layer, which gets updated  as the model reads one word at a time.

ASK THE MODEL TO LEARN THE RIGHT REPRESENTATION FOR A WORD ON HIS OWN.

Taking in part of a sentence and predicting the next word is just the tip of the iceberg for NLP.

The representations of words that our model learns for one kind of task might not for others.
