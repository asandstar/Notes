Neural networks handle mistake by using an algorithm called backpropagation to make sure all the neurons that contributed to an error get their math adjusted, and we’ll unpack this a bit later.
optimization:the task of finding the best weights for a neural network architecture. 

Linear regresion

Background: As we consider more features, we add more dimensions to the graph, the optimization problem gets trickier and fitting the training data is tougher. 

Solution: a neural network connect together many simple neurons with weights and learn to solve complicated problems, where the line of best fit becomes a weird multi-dimensional function.

Loss function: In some neural networks, the output layer may have a lot of neurons ,therefore the difference between the predicted answer and the correct answer is more than just one number.  SO THE ERROR IS THE LOSS FUNCTION.

Adjust the neural network’s weight so that the next time the final output will be more accurate. 

GOAL: to find the best combination of weights to get the lowest error. 

Always looking for more creative ways to explore solutions, try different combinations of weights, and minimize the loss function as we train neural networks. 

OVERFITTING: We give a neural network some new data that doesn’t adhere to these silly correlations, then it will probably make strange errors. 

Prevent overfitting? keep the neural network simple.

- Training a neural network isn’t just a bunch of math, we need to consider how to best represent various problems as features in AI systems, and to think carefully about what mistakes these programs might make.

How to find the optimal solution?

1.Trying different random starting points to be sure that the neural network isn’t getting stuck at a locally optimal solution. 

2.Having a team of explorers that start from different locations and explore the jungle simultaneously. Namely, exploring different solutions at the same time on the same neural network. (ESPECIALLY USEFUL WHEN HAVING A GIANT COMPUTER)

3.Adjusting the explorer’s step size.

Learning rate: how much the neuron weights get adjusted every time backpropagation happens.
